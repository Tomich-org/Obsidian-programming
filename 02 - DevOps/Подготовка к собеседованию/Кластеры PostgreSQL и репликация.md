В PostgreSQL **single instance** – это одиночный сервер БД. **Кластер** БД подразумевает несколько  серверов, настроенных на репликацию, для отказоустойчивости и масштабируемости. Обычно  один сервер является **master (primary)**, принимающим все записи, а другие – **replica (standby)** –   получают от master все изменения через лог репликации WAL. Это один из способов сделать отказоустойчивый кластер: при падении мастера одну из реплик можно **promote** (сделать новым  мастером) .  

Есть два вида репликации: - **Physical (streaming) replication**: мастером настраивается режим  
`wal_level = replica`, создаётся репликационный пользователь. Резервный сервер   подключается и получает WAL-записи по сети. Реплика может быть доступна только для чтения   во время работы мастера (hot standby).  **Logical replication**: (начиная с PG 10+) мастер публикует изменения (pub), реплика  подписывается (sub). Позволяет реплицировать отдельные таблицы.  

**Установка**: На основном сервере редактируем `postgresql.conf` (включаем `wal_level`,   настраиваем `max_wal_senders` ) и `pg_hba.conf` (разрешаем доступ реплике). На реплике  
создаётся файл `recovery.conf` (или в PG12+ – `standby.signal` и параметры в  `postgresql.conf` ), где указывают primary_conninfo . После запуска реплика подтягивает  данные.  

**Тип подчинённости**: В классическом сценарии – Master/Slave (один мастер, один или несколько  слейвов). В некоторых решениях поддерживаются и master-master  двунаправленная)   репликация, но PostgreSQL из коробки так не делает (есть сторонние решения).  

**Работа клиентов**: Обычно клиенты посылают все запросы на мастер, если нужны изменения. 
Чтения можно направлять на реплики (с помощью балансировщика запросов – pgpool,   pgbouncer, встроенных механизмов самой программы). Таким образом, нагрузку на чтение  
можно распределить. Если мастер упал и сделана промоция реплики, нужно перенастроить  
приложения на нового мастера.  

**Пример**: «У нас была схема «master – 2 replicas». Мастер обслуживал INSERT/UPDATE, а SELECT  запросы к историческим данным и архиву мы делегировали репликам. В приложении прописали   поддержку failover через pgbouncer. При обновлении схемы (DDL) нужно было синхронно  применять изменения на репликах, либо сначала сломать реплику, выполнить миграции, потом  «расшарить».